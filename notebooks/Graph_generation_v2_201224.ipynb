{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8f06d2c-b6ee-41b4-b875-d7c701cb6954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyteomics import fasta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383853d8-35ff-4de4-acf7-b6dcac3da4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data path\n",
    "datadir = os.path.abspath(\"../../Data/\")\n",
    "\n",
    "#Datasets\n",
    "OT_targets    = datadir+\"/OpenTargets/targets\" #OpenTargets | \"targets\" data\n",
    "OT_directevi  = datadir+\"/OpenTargets/associationByOverallDirect\" #OpenTargets | Disease association - Direct evidence data\n",
    "OT_indireevi  = datadir+\"/OpenTargets/associationByOverallIndirect\" #OpenTargets | Disease association - Indirect evidence data\n",
    "diseasedb     = datadir+\"/OpenTargets/Disease\" #OpenTargets \"Disease\" intel\n",
    "\n",
    "Uniprot_hsap  = datadir+\"/Uniprotdb/human_proteome/UP000005640_9606.fasta\" #Uniprot h. sapiens proteome\n",
    "Unisprot             = datadir+\"/Uniprotdb/uniprot_sprot_human.dat\" #Uniprot core dataset, contains functional intel\n",
    "Genemap       = datadir+\"/Uniprotdb/HUMAN_9606_idmapping.dat\" #Uniprot gene ID mapping\n",
    "\n",
    "Orpha_en      = datadir+\"/Disease_ontology/orpha_en_product1.json\" #Orphanet rare disease xef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164fc539-42d5-40c2-ac51-439384f71260",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e291ac4-2801-4ecc-ba4a-78ca10cdcb4d",
   "metadata": {},
   "source": [
    "## Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01b42a5-95ee-49e2-b549-2e1d360d7975",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def genUniprot(unidata, genein):\n",
    "    #Generate Uniprot <> Gene dictionary\n",
    "    with open(genein) as f:\n",
    "        read = f.readlines()\n",
    "    unigene = {}\n",
    "    for i in read:\n",
    "        splitr = i.split(\"\\t\")\n",
    "        if splitr[1] == \"Gene_Name\" or splitr[1] == \"Gene_Synonym\":\n",
    "            unigene.setdefault(splitr[0], [])\n",
    "            unigene[splitr[0]].append(splitr[2].strip(\"\\n\"))\n",
    "\n",
    "    #Generate df of human proteome & dic of gene names to uniprot ids\n",
    "    unidic    = {}\n",
    "    fastain   = fasta.UniProt(unidata)\n",
    "    count = 0\n",
    "    for uni in fastain:\n",
    "        count += 1\n",
    "        unid = uni[0]['id']\n",
    "        name = uni[0]['name']\n",
    "        try:\n",
    "            gene = unigene[unid][0]\n",
    "        except:\n",
    "            gene = uni[0]['gene_id']\n",
    "        seq  = uni[1]\n",
    "        unidic.setdefault(unid, {\"Uniprot\": unid, \"Gene\": gene, \"Name\": name, \"Seq\": seq})\n",
    "\n",
    "    unidf = pd.DataFrame.from_dict(unidic, orient='index')\n",
    "\n",
    "    return unidf\n",
    "\n",
    "def parseOTtarget(data_dir, df):\n",
    "    #Parse OpenTargets \"targets\" intel, return df\n",
    "    #Add new intel cols to df\n",
    "    intelcol = {\"Ensembl_gene\": 'id', \\\n",
    "                \"Ensembl_trans\": 'transcriptIds', \\\n",
    "                \"Description\": \"functionDescriptions\", \\\n",
    "                \"Subcell_loc\": \"subcellularLocations\", \\\n",
    "                \"DB_entries\": \"dbXrefs\", \\\n",
    "                \"Bio_path\": \"pathways\", \\\n",
    "                \"Tx_approaches\": \"tractability\"}\n",
    "    for col in intelcol:\n",
    "        df[col] = [list() for x in range(len(df.index))]\n",
    "    data = []\n",
    "    #Read in multi part .json to data list \n",
    "    for file in os.listdir(data_dir):\n",
    "        path = data_dir + \"/\" + str(file).replace(\"._\", \"\")\n",
    "        with open(path, \"r\") as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "                \n",
    "    #Retrieve intel from protein coding entries, annotate df\n",
    "    ensemtouni = {}\n",
    "    for entry in data:\n",
    "        try:\n",
    "            entid = entry['proteinIds'][0]['id']\n",
    "        except:\n",
    "            entid = \"\"\n",
    "\n",
    "        if entid in df.index:\n",
    "            for j in intelcol:\n",
    "                if intelcol[j] in entry and j != \"Bio_path\":\n",
    "                    df.loc[entid, j].append(entry[intelcol[j]])\n",
    "                    #Simplify biological pathways for easier querying\n",
    "                elif intelcol[j] in entry and j == \"Bio_path\":\n",
    "                    for path in entry[intelcol[j]]:\n",
    "                        df.loc[entid, j].append([path['pathwayId'], path['pathway']])\n",
    "                elif j == \"Ensembl_gene\":\n",
    "                    ensemtouni.setdefault(entry[intelcol[j]], entid) \n",
    "    \n",
    "    return df, ensemtouni\n",
    "\n",
    "def parseOTdisease(dataloc):\n",
    "    #Parse OT disease <> protein evidence\n",
    "    data = {}\n",
    "    for file in os.listdir(dataloc):\n",
    "        if \".json\" in str(file):\n",
    "            with open(dataloc + \"/\" + str(file).replace(\"._\", \"\")) as f:\n",
    "                for line in f.readlines():\n",
    "                    linedict = json.loads(line)\n",
    "                    data.setdefault(linedict[\"diseaseId\"], [linedict])\n",
    "                    data[linedict[\"diseaseId\"]].append(linedict)\n",
    "    return data\n",
    "\n",
    "def findDisease(database):\n",
    "    #Parse high level OT disease intel\n",
    "    data = {}\n",
    "    count = 0\n",
    "    for file in os.listdir(database):\n",
    "        if \".json\" in str(file):\n",
    "            count += 1\n",
    "            try:\n",
    "                with open(database + \"/\" + str(file)) as f:\n",
    "                    for line in f.readlines():\n",
    "                        linedict = json.loads(line)\n",
    "                        data.setdefault(linedict[\"id\"], linedict)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return data\n",
    "\n",
    "def funcGen(unisprot):\n",
    "    #Return a dictionary of Uniprot to function annotations\n",
    "    outdic = {}\n",
    "    with open(unisprot, \"r\") as f:\n",
    "        infile = f.read().replace(\";\", \" \")\n",
    "        splitin= infile.split(\"//\\n\")\n",
    "    #Iterate per entry, split block, retrieve Uniprot + individual functions\n",
    "    for uni in splitin:\n",
    "        count = 0\n",
    "        allfunc = [line for line in uni.split(\"\\n\") if line.startswith('FT')]\n",
    "        allac = [line for line in uni.split(\"\\n\") if line.startswith('AC')]\n",
    "        try:\n",
    "            uniprot = re.search(r\"\\bAC\\s+([A-Za-z0-9]+)\", uni).group().split()[1]\n",
    "        except:\n",
    "            uniprot = \"\"\n",
    "        funcdic = {}\n",
    "        func = {}\n",
    "        curannot = \"\"\n",
    "        #Iterate over functions, append to out dic\n",
    "        for i in allfunc:\n",
    "            match = re.match(r\"^\\S+(\\s+)\\S+\", i)\n",
    "            if len(match.group(1)) == 3:\n",
    "                funcdic.setdefault(count, func)\n",
    "                curannot = \"\"\n",
    "                spliti = i.split()\n",
    "                count += 1\n",
    "                nums = re.findall(r'\\d+', spliti[-1])\n",
    "                nums = [int(num) for num in nums]\n",
    "                func = {\"type\": spliti[1].replace(\";\", \" \"), \"range\": nums, \"note\": \"\", \"evidence\": \"\", \"id\": \"\"}\n",
    "            else:\n",
    "                if re.search(r\"(?<=\\/)\\w+=+\", i) != None:\n",
    "                    search = re.search(r\"(?<=\\/)\\w+=+\", i)\n",
    "                    curannot = search.group()[:-1]\n",
    "                    func[curannot] = [i.split(\"=\")[1].replace(\"\\\"\", \"\")] \n",
    "                    spliti = i.split(\"/id=\")\n",
    "                else:\n",
    "                    func[curannot] = func[curannot][0]+\" \".join(i.split()[1:])\n",
    "        outdic.setdefault(uniprot, funcdic)\n",
    "    return outdic\n",
    "\n",
    "def parseOrpha(datain):\n",
    "    ##TO-DO\n",
    "        #Alter output to df with cols = ontologies    \n",
    "    #Read in Orphanet json, return dataframe of Orpha code / Name / other database crossreferences\n",
    "    with open(datain) as f:\n",
    "        dataj = json.load(f)\n",
    "    outdic = {}\n",
    "    for i in dataj[\"JDBOR\"][0][\"DisorderList\"][0][\"Disorder\"]:\n",
    "        orpha = i[\"OrphaCode\"]\n",
    "        nam   = i[\"Name\"][0][\"label\"]\n",
    "        xrefs = {}\n",
    "        try:\n",
    "            for j in i[\"ExternalReferenceList\"][0]['ExternalReference']:\n",
    "                xrefs.setdefault(j['Source'], {\"ID\": j['id'], \"Mapping\": j['DisorderMappingRelation'][0][\"Name\"][0][\"label\"]})\n",
    "        except:\n",
    "            pass\n",
    "        outdic.setdefault(orpha, {\"Name\": nam, \"Xref\": xrefs})\n",
    "\n",
    "    outdf = pd.DataFrame.from_dict(outdic)\n",
    "\n",
    "    return outdf.transpose() \n",
    "\n",
    "def assocUniprotDisease(datadic, ensgenetouni, diseasexref):\n",
    "    #Generate dictionary linking Uniprot ID to disease associations + scores\n",
    "    uniprotdisease = {}\n",
    "    for i in datadic:\n",
    "        for ent in datadic[i]:\n",
    "            try:\n",
    "                Orpha   = int(ent['diseaseId'].split(\"_\")[1])\n",
    "                Score   = round(ent['score'], 3)\n",
    "                Uniprot = ensgenetouni[ent['targetId']]\n",
    "                Disnam  = diseasexref.loc[str(Orpha)][\"Name\"]\n",
    "                uniprotdisease.setdefault(Uniprot, {Orpha: {\"Disease\": Disnam, \"Score\": Score}})\n",
    "                uniprotdisease[Uniprot][Orpha] = {\"Disease\": Disnam, \"Score\": Score}\n",
    "            except:\n",
    "                pass\n",
    "    return uniprotdisease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7afdee-712b-48fa-9668-8fd86927d830",
   "metadata": {},
   "source": [
    "## Node / Edge generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f2cb48e-fb69-40b8-bf23-38a9f56c62e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def genProtNode(df):\n",
    "    #Generate Uniprot Protein node\n",
    "    count = 0\n",
    "    outcsv = [[\"id:ID\", \":LABEL\", \"uniprot\", \"gene\", \"name\", \"seq\", \"ens_gene\", \"ens_trans\", \"description\", \"db_entries\", \"bio_path\", \"tx_approach\"]]\n",
    "    nodecodec = {}\n",
    "    for index, row in df.iterrows():\n",
    "        count += 1\n",
    "        outcsv.append([\"P\"+str(count), \"Protein\", row[\"Uniprot\"], row['Gene'], row['Name'], row['Seq'], row['Ensembl_gene'], row['Ensembl_trans'],\\\n",
    "                        row['Description'], row['DB_entries'], row['Bio_path'], row['Tx_approaches']])\n",
    "        nodecodec.setdefault(index, \"P\"+str(count))\n",
    "    return outcsv, nodecodec\n",
    "\n",
    "def genFuncNode(funcdic):\n",
    "    #Generate Uniprot Function node\n",
    "    count = 0\n",
    "    outcsv = [[\"id:ID\", \":LABEL\", \"type\",  \"range\", \"alt_id\", \"evidence\", \"note\"]]\n",
    "    nodecodec = {}\n",
    "    for uni in funcdic:\n",
    "        for func in funcdic[uni]: \n",
    "            if len(funcdic[uni][func]) > 0:\n",
    "                count += 1\n",
    "                outcsv.append([\"F\"+str(count), \"Function\", funcdic[uni][func][\"type\"], funcdic[uni][func][\"range\"], funcdic[uni][func][\"id\"], funcdic[uni][func][\"evidence\"], funcdic[uni][func][\"note\"]])\n",
    "                nodecodec.setdefault(\"F\"+str(count), uni)\n",
    "    return outcsv, nodecodec\n",
    "\n",
    "def genDisNode(disdic, disedge):\n",
    "    #Generate OpenTarget disease node\n",
    "    count = 0\n",
    "    outcsv = [[\"id:ID\", \":LABEL\", \"id_ont\", \"name\", \"description\"]]\n",
    "    nodecodec = {}\n",
    "    for dis in disdic:\n",
    "        if dis in disedge:\n",
    "            try:\n",
    "                desc = disdic[dis][\"description\"]\n",
    "            except:\n",
    "                desc = \"\"\n",
    "            count += 1\n",
    "            outcsv.append([\"D\"+str(count), \"Disease\", dis, disdic[dis][\"name\"], desc.replace(\"\\n\", \" \")])\n",
    "            nodecodec.setdefault(dis, \"D\"+str(count))\n",
    "    return outcsv, nodecodec\n",
    "\n",
    "def genProtFuncEdge(funcod, protcod):\n",
    "    #Generate Protein <> Functional edges\n",
    "    outcsv = [[\":START_ID\",\":END_ID\", \":TYPE\"]]\n",
    "    count = 0\n",
    "    for i in funcod:\n",
    "        if funcod[i] in protcod:\n",
    "            outcsv.append([i, protcod[funcod[i]], \"is_feature\"])\n",
    "\n",
    "    return outcsv\n",
    "\n",
    "def genDisProtEdge(disdic, protcod, discod, ensdic):\n",
    "    #Generate Disease <> Functional edges\n",
    "    outcsv = [[\":START_ID\",\":END_ID\", \":TYPE\", \"score\"]]\n",
    "    for i in disdic:\n",
    "        for j in disdic[i]:\n",
    "            if j[\"targetId\"] in ensdic:\n",
    "                outcsv.append([protcod[ensdic[j[\"targetId\"]]], discod[i], \"direct_evidence\", float(j[\"score\"])])\n",
    "    return outcsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e492938-4012-4521-8a2e-a50016afba1c",
   "metadata": {},
   "source": [
    "## Graph read / write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd30b83-7738-4cb3-a68d-5e8674444379",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def nodeToCSVprep(genenode, diseasenode):\n",
    "    #Generate out lists for .csv out\n",
    "    geneout = [[\"id:ID\", \":LABEL\", \"name\", \"ensemble_id\"]]\n",
    "    for i in genenode:\n",
    "        geneout.append([genenode[i][\"ID\"], \"Gene\", i, genenode[i][\"Ensemble\"]])\n",
    "        \n",
    "    disout = [[\"id:ID\", \":LABEL\", \"name\", \"uri\"]]\n",
    "    for i in diseasenode:\n",
    "        disout.append([diseasenode[i][\"ID\"], \"Disease\", i, diseasenode[i][\"URI\"]])\n",
    "        \n",
    "    return geneout, disout\n",
    "\n",
    "def writecsv(infile, outfile):\n",
    "    #Write out csv\n",
    "    with open(outfile, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\";\")\n",
    "        writer.writerows(infile)\n",
    "\n",
    "def run_query(query):\n",
    "    #Submit query to graph database, retrieve output as df\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query)\n",
    "        rows = [record.data() for record in result]\n",
    "        df = pd.DataFrame(rows)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed1762-d17c-44c1-b61f-6b02b231b926",
   "metadata": {},
   "source": [
    "# Main script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9353e-2b5b-4705-af45-0369f4bb564a",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73692a0-75ca-44fa-9a2a-ae20fc004a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken | Uniprot initial proteome =  0.98\n",
      "Time taken | OpenTargets targets intel =  3.86\n",
      "Time taken | Ensemble <> Uniprot | Gene <> Uniprot lookup dic =  0.42\n",
      "Time taken | Uniprot function intel =  5.8\n",
      "Time taken | OpenTargets disease intel =  0.12\n",
      "Time taken | Rare disese xref =  1.1\n",
      "Time taken | OpenTargets disease intel =  13.46\n",
      "Time taken | Uniprot disease association =  64.76\n"
     ]
    }
   ],
   "source": [
    "#Read in Uniprot proteome intel\n",
    "start = time.time()\n",
    "unidf = genUniprot(Uniprot_hsap, Genemap)\n",
    "print(\"Time taken | Uniprot initial proteome = \", round(time.time() - start, 2))\n",
    "\n",
    "#Read in OT target intel - DONE\n",
    "start = time.time()\n",
    "unidf, ensemtouni = parseOTtarget(OT_targets, unidf)\n",
    "print(\"Time taken | OpenTargets targets intel = \", round(time.time() - start, 2))\n",
    "\n",
    "#Lookup dic of ensemble gene ids to uniprot\n",
    "start = time.time()\n",
    "ensgenetouni = {}\n",
    "genetouni    = {}\n",
    "for index, row in unidf.iterrows():\n",
    "    val = row[\"Ensembl_gene\"]\n",
    "   # print(val)\n",
    "    for i in row[\"Ensembl_gene\"]:\n",
    "        ensgenetouni.setdefault(i.replace(\"\\'\", \"\"), index)\n",
    "    genetouni.setdefault(row[\"Gene\"], index)\n",
    "print(\"Time taken | Ensemble <> Uniprot | Gene <> Uniprot lookup dic = \", round(time.time() - start, 2))\n",
    "\n",
    "#Read in OT functional intel \n",
    "start = time.time()\n",
    "funcdic = funcGen(Unisprot)\n",
    "print(\"Time taken | Uniprot function intel = \", round(time.time() - start, 2))\n",
    "\n",
    "#Read in OT disease intel\n",
    "start = time.time()\n",
    "diseasego = findDisease(diseasedb) \n",
    "print(\"Time taken | OpenTargets disease intel = \", round(time.time() - start, 2))\n",
    "\n",
    "#Read in disease associations\n",
    "start = time.time()\n",
    "OT_direct = parseOTdisease(OT_directevi)\n",
    "OT_indir  = parseOTdisease(OT_indireevi)\n",
    "print(\"Time taken | OpenTargets disease intel = \", round(time.time() - start, 2))\n",
    "\n",
    "#Generate dictionaries of Uniprot to disease assocations\n",
    "start = time.time()\n",
    "dirdic = assocUniprotDisease(OT_direct, ensgenetouni, diseasexref)\n",
    "inddic = assocUniprotDisease(OT_indir, ensgenetouni, diseasexref)\n",
    "print(\"Time taken | Uniprot disease association = \", round(time.time() - start, 2))\n",
    "\n",
    "#Generate rare disease crossreference\n",
    "start = time.time()\n",
    "diseasexref = parseOrpha(Orpha_en)\n",
    "print(\"Time taken | Rare disese xref = \", round(time.time() - start, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d62ac2-ffef-449a-8dc6-3c3591ffd3ba",
   "metadata": {},
   "source": [
    "## Node / Edge generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f708a285-93e5-4e56-bcba-882b3cf56f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate nodes / edges csv\n",
    "protnodes, protcodec = genProtNode(unidf)\n",
    "funcnodes, funccodec = genFuncNode(funcdic)\n",
    "disnodes, discodec   = genDisNode(diseasego, OT_direct)\n",
    "profuncedge          = genProtFuncEdge(funccodec, protcodec)\n",
    "disprotedge          = genDisProtEdge(OT_direct, protcodec, discodec, ensgenetouni)\n",
    "#transnod, transedge  = genscRNANodeEdge(scdf, genetouni, protcodec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f3cfac1-442f-46ec-846c-cdc71841b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out node + edge csv files\n",
    "if os.path.exists(\"./Results\") == False:\n",
    "    os.mkdir(\"./Results\")\n",
    "\n",
    "#Write out nodes / edges csv\n",
    "writecsv(protnodes, \"./Results/protnodes.csv\")\n",
    "writecsv(disnodes, \"./Results/disnodes.csv\")\n",
    "writecsv(funcnodes, \"./Results/funcnodes.csv\")\n",
    "writecsv(profuncedge, \"./Results/profuncedge.csv\")\n",
    "writecsv(disprotedge, \"./Results/disprotedge.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951a66a-ff6d-408e-ba51-902263df6260",
   "metadata": {},
   "source": [
    "## Start graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dee4bec6-04c9-4b7c-8254-5111c8a0d466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph stopped\n",
      "Database import successful!\n",
      "Graph started\n"
     ]
    }
   ],
   "source": [
    "#Password\n",
    "with open(\"./pass_ent.txt\", \"r\") as f:\n",
    "    password = f.read()\n",
    "\n",
    "curdir = os.path.abspath(\"./Results/\")\n",
    "#Commands for stopping / generating / starting neo4j\n",
    "cmd0 = [\"sudo\", \"-S\", \"neo4j\", \"stop\"]\n",
    "cmd1 = [ \\\n",
    "        \"sudo\", \"-S\", \"neo4j-admin\", \"database\", \"import\", \"full\", \"neo4j\", \\\n",
    "        \"--nodes=\"+ curdir+\"/protnodes.csv\", \\\n",
    "        \"--nodes=\"+ curdir+\"/disnodes.csv\", \\\n",
    "        \"--nodes=\"+ curdir+\"/funcnodes.csv\", \\\n",
    "        \"--relationships=\"+ curdir+\"/profuncedge.csv\", \\\n",
    "        \"--relationships=\"+ curdir+\"/disprotedge.csv\", \\\n",
    "        \"--delimiter=;\", \\\n",
    "        \"--array-delimiter=|\", \\\n",
    "        \"--overwrite-destination\", \\\n",
    "        \"--verbose\"\n",
    "        ]\n",
    "cmd2 = [\"sudo\", \"-S\", \"neo4j\", \"start\", \"--verbose\"]\n",
    "\n",
    "#Stop server\n",
    "result = subprocess.run(cmd0, input=password+\"\\n\", capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"Graph stopped\")\n",
    "else:\n",
    "    print(f\"Stop failed with error:\\n{result.stderr}\")\n",
    "\n",
    "#Generate graph\n",
    "result = subprocess.run(cmd1, input=password+\"\\n\", capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"Database import successful!\")\n",
    "else:\n",
    "    print(f\"Import failed with error:\\n{result.stderr}\")\n",
    "\n",
    "#Start graph\n",
    "result = subprocess.run(cmd2, input=password+\"\\n\", capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"Graph started\")\n",
    "else:\n",
    "    print(f\"Start failed with error:\\n{result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022a45d-9ae0-46bc-b0ec-089c40d4ea45",
   "metadata": {},
   "source": [
    "## Run cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8455aa8c-2788-4a9a-9099-86ceafa3d1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
